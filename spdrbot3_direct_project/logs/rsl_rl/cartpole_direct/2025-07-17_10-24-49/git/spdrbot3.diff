--- git status ---
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/skrl_ppo_cfg.yaml
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
index 44bdf1e..864f3ca 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
@@ -1,4 +1,5 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers[](https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
index b7e7cc9..4026dab 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
@@ -10,15 +10,15 @@ from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlPpoActorCriticCfg, R
 
 @configclass
 class PPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 16
-    max_iterations = 150
+    num_steps_per_env = 24
+    max_iterations = 20000
     save_interval = 50
     experiment_name = "cartpole_direct"
     empirical_normalization = False
     policy = RslRlPpoActorCriticCfg(
         init_noise_std=1.0,
-        actor_hidden_dims=[32, 32],
-        critic_hidden_dims=[32, 32],
+        actor_hidden_dims=[64, 64],
+        critic_hidden_dims=[64, 64],
         activation="elu",
     )
     algorithm = RslRlPpoAlgorithmCfg(
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/skrl_ppo_cfg.yaml b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/skrl_ppo_cfg.yaml
index 02c62c2..1698018 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/skrl_ppo_cfg.yaml
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/skrl_ppo_cfg.yaml
@@ -15,7 +15,7 @@ models:
     network:
       - name: net
         input: STATES
-        layers: [32, 32]
+        layers: [64, 64]
         activations: elu
     output: ACTIONS
   value:  # see deterministic_model parameters
@@ -24,7 +24,7 @@ models:
     network:
       - name: net
         input: STATES
-        layers: [32, 32]
+        layers: [64, 64]
         activations: elu
     output: ONE
 
@@ -76,5 +76,5 @@ agent:
 # https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
 trainer:
   class: SequentialTrainer
-  timesteps: 4800
+  timesteps: 20000
   environment_info: log
\ No newline at end of file
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
index b50c7a5..c2a9d5b 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
@@ -1,21 +1,23 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# spdrbot3_env.py
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
 
 from __future__ import annotations
 
-import math
+import gymnasium as gym
 import torch
-from collections.abc import Sequence
 
 import isaaclab.sim as sim_utils
 from isaaclab.assets import Articulation
 from isaaclab.envs import DirectRLEnv
-from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-from isaaclab.utils.math import sample_uniform
+from isaaclab.sensors import ContactSensor
+from isaaclab.markers import VisualizationMarkersCfg, VisualizationMarkers
+from isaaclab.utils.math import quat_rotate, quat_from_angle_axis
 
 from .spdrbot3_env_cfg import Spdrbot3EnvCfg
+from pxr import PhysxSchema
 
 
 class Spdrbot3Env(DirectRLEnv):
@@ -24,112 +26,273 @@ class Spdrbot3Env(DirectRLEnv):
     def __init__(self, cfg: Spdrbot3EnvCfg, render_mode: str | None = None, **kwargs):
         super().__init__(cfg, render_mode, **kwargs)
 
-        self._cart_dof_idx, _ = self.robot.find_joints(self.cfg.cart_dof_name)
-        self._pole_dof_idx, _ = self.robot.find_joints(self.cfg.pole_dof_name)
+        # Joint position command (deviation from default joint positions)
+        self._actions = torch.zeros(self.num_envs, gym.spaces.flatdim(self.single_action_space), device=self.device)
+        self._previous_actions = torch.zeros(
+            self.num_envs, gym.spaces.flatdim(self.single_action_space), device=self.device
+        )
+
+        # X/Y linear velocity commands (no yaw)
+        self._commands = torch.zeros(self.num_envs, 2, device=self.device)
+
+        # History buffer for joint positions
+        self._joint_pos_buffer = torch.zeros(self.num_envs, self.cfg.history_length, 12, device=self.device)
+
+        # Time step counter for phase
+        self._time_step_counter = torch.zeros(self.num_envs, device=self.device)
+
+        # Get specific body indices
+        self._base_id, _ = self._contact_sensor.find_bodies("base_link")
+        self._feet_ids, _ = self._contact_sensor.find_bodies(".*_4_1")  # Adjusted to match body names like arm_a_4_1 etc.
+        self._die_body_ids, _ = self._contact_sensor.find_bodies(["arm_a_1_1", "arm_a_2_1", "arm_a_3_1", "arm_a_4_1"])  # Bodies that trigger die on contact
+
+        # Logging
+        self._episode_sums = {
+            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
+            for key in [
+                "track_lin_vel_xy_exp",
+                "track_ang_vel_z_exp",
+                "lin_vel_z_l2",
+                "ang_vel_xy_l2",
+                "joint_vel_l2",  # Added for joint_vel_penalty
+                "dof_torques_l2",
+                "dof_acc_l2",
+                "action_rate_l2",
+                "flat_orientation_l2",
+                "power_penalty",
+            ]
+        }
 
-        self.joint_pos = self.robot.data.joint_pos
-        self.joint_vel = self.robot.data.joint_vel
 
     def _setup_scene(self):
-        self.robot = Articulation(self.cfg.robot_cfg)
-        # add ground plane
-        spawn_ground_plane(prim_path="/World/ground", cfg=GroundPlaneCfg())
+        self._robot = Articulation(self.cfg.robot)
+        self.scene.articulations["robot"] = self._robot
+        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
+        self.scene.sensors["contact_sensor"] = self._contact_sensor
+        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
+        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
+        self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
         # clone and replicate
         self.scene.clone_environments(copy_from_source=False)
         # we need to explicitly filter collisions for CPU simulation
         if self.device == "cpu":
-            self.scene.filter_collisions(global_prim_paths=[])
-        # add articulation to scene
-        self.scene.articulations["robot"] = self.robot
+            self.scene.filter_collisions(global_prim_paths=[self.cfg.terrain.prim_path])
         # add lights
         light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
         light_cfg.func("/World/Light", light_cfg)
 
-    def _pre_physics_step(self, actions: torch.Tensor) -> None:
-        self.actions = actions.clone()
+        # Setup visualization markers for commanded (red cone) and actual (green cone) directions
+        commanded_marker_cfg = VisualizationMarkersCfg(
+            prim_path="/Visuals/CommandedDirections",
+            markers={
+                "cone": sim_utils.ConeCfg(
+                    radius=0.01,  # Thin base
+                    height=0.2,  # Long height
+                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(1.0, 0.0, 0.0)),  # Red
+                )
+            },
+        )
+        self._commanded_markers = VisualizationMarkers(commanded_marker_cfg)
 
-    def _apply_action(self) -> None:
-        self.robot.set_joint_effort_target(self.actions * self.cfg.action_scale, joint_ids=self._cart_dof_idx)
+        actual_marker_cfg = VisualizationMarkersCfg(
+            prim_path="/Visuals/ActualDirections",
+            markers={
+                "cone": sim_utils.ConeCfg(
+                    radius=0.01,
+                    height=0.2,
+                    visual_material=sim_utils.PreviewSurfaceCfg(diffuse_color=(0.0, 1.0, 0.0)),  # Green
+                )
+            },
+        )
+        self._actual_markers = VisualizationMarkers(actual_marker_cfg)
+
+    def _pre_physics_step(self, actions: torch.Tensor):
+        self._actions = actions.clone()
+        self._processed_actions = self.cfg.action_scale * self._actions + self._robot.data.default_joint_pos
+
+    def _apply_action(self):
+        self._robot.set_joint_position_target(self._processed_actions)
 
     def _get_observations(self) -> dict:
+        self._previous_actions = self._actions.clone()  # Still update for rewards, even if not in obs
+
+        # Update history buffer: shift left and append current joint_pos
+        self._joint_pos_buffer = torch.roll(self._joint_pos_buffer, shifts=-1, dims=1)
+        self._joint_pos_buffer[:, -1] = self._robot.data.joint_pos
+
+        # Flatten history
+        joint_history_flat = self._joint_pos_buffer.reshape(self.num_envs, -1)
+
+        # Compute phase
+        phase1 = 2 * torch.pi * (self._time_step_counter % self.cfg.phase1_period) / self.cfg.phase1_period
+
+
+        sin_phase = torch.sin(phase1).unsqueeze(-1)
+        cos_phase = torch.cos(phase1).unsqueeze(-1)
+
         obs = torch.cat(
-            (
-                self.joint_pos[:, self._pole_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_vel[:, self._pole_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_pos[:, self._cart_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_vel[:, self._cart_dof_idx[0]].unsqueeze(dim=1),
-            ),
+            [
+                self._commands,
+                joint_history_flat,
+                sin_phase,
+                cos_phase,
+            ],
             dim=-1,
         )
         observations = {"policy": obs}
         return observations
 
     def _get_rewards(self) -> torch.Tensor:
-        total_reward = compute_rewards(
-            self.cfg.rew_scale_alive,
-            self.cfg.rew_scale_terminated,
-            self.cfg.rew_scale_pole_pos,
-            self.cfg.rew_scale_cart_vel,
-            self.cfg.rew_scale_pole_vel,
-            self.joint_pos[:, self._pole_dof_idx[0]],
-            self.joint_vel[:, self._pole_dof_idx[0]],
-            self.joint_pos[:, self._cart_dof_idx[0]],
-            self.joint_vel[:, self._cart_dof_idx[0]],
-            self.reset_terminated,
-        )
-        return total_reward
+        # linear velocity tracking (only x/y)
+        lin_vel_error = torch.sum(torch.square(self._commands - self._robot.data.root_lin_vel_b[:, :2]), dim=1)
+        lin_vel_error_mapped = torch.exp(-lin_vel_error / 0.25)
+        # yaw rate tracking (penalize deviation from 0)
+        yaw_command = torch.zeros(self.num_envs, device=self.device)
+        yaw_rate_error = torch.square(yaw_command - self._robot.data.root_ang_vel_b[:, 2])
+        yaw_rate_error_mapped = torch.exp(-yaw_rate_error / 0.25)
+        # z velocity tracking
+        z_vel_error = torch.square(self._robot.data.root_lin_vel_b[:, 2])
+        # angular velocity x/y
+        ang_vel_error = torch.sum(torch.square(self._robot.data.root_ang_vel_b[:, :2]), dim=1)
+        # joint velocities (added penalty term)
+        joint_vel_error = torch.sum(torch.square(self._robot.data.joint_vel), dim=1)
+        # joint torques
+        joint_torques = torch.sum(torch.square(self._robot.data.applied_torque), dim=1)
+        # # joint acceleration
+        joint_accel = torch.sum(torch.square(self._robot.data.joint_acc), dim=1)
+        # action rate
+        action_rate = torch.sum(torch.square(self._actions - self._previous_actions), dim=1)
+        # flat orientation
+        flat_orientation = torch.sum(torch.square(self._robot.data.projected_gravity_b[:, :2]), dim=1)
+        joint_power = torch.sum(torch.abs(self._robot.data.applied_torque * self._robot.data.joint_vel), dim=1)
 
-    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
-        self.joint_pos = self.robot.data.joint_pos
-        self.joint_vel = self.robot.data.joint_vel
+        rewards = {
+            "track_lin_vel_xy_exp": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale * self.step_dt,
+            "track_ang_vel_z_exp": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale * self.step_dt,
+            "lin_vel_z_l2": z_vel_error * self.cfg.z_vel_reward_scale * self.step_dt,
+            "ang_vel_xy_l2": ang_vel_error * self.cfg.ang_vel_reward_scale * self.step_dt,
+            "joint_vel_l2": joint_vel_error * self.cfg.joint_vel_penalty_scale * self.step_dt,  # Added
+            "dof_torques_l2": joint_torques * self.cfg.joint_torque_reward_scale * self.step_dt,
+            "dof_acc_l2": joint_accel * self.cfg.joint_accel_reward_scale * self.step_dt,
+            "action_rate_l2": action_rate * self.cfg.action_rate_reward_scale * self.step_dt,
+            "flat_orientation_l2": flat_orientation * self.cfg.flat_orientation_reward_scale * self.step_dt,
+            "power_penalty": joint_power * self.cfg.power_penalty_scale * self.step_dt,
+        }
+        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)
+        # Logging
+        for key, value in rewards.items():
+            self._episode_sums[key] += value
+        return reward
 
+    def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
         time_out = self.episode_length_buf >= self.max_episode_length - 1
-        out_of_bounds = torch.any(torch.abs(self.joint_pos[:, self._cart_dof_idx]) > self.cfg.max_cart_pos, dim=1)
-        out_of_bounds = out_of_bounds | torch.any(torch.abs(self.joint_pos[:, self._pole_dof_idx]) > math.pi / 2, dim=1)
-        return out_of_bounds, time_out
+        net_contact_forces = self._contact_sensor.data.net_forces_w_history
+        died = torch.any(
+            torch.max(torch.norm(net_contact_forces[:, :, self._die_body_ids], dim=-1), dim=1)[0] > 1.0, dim=1
+        )
+        return died, time_out
 
-    def _reset_idx(self, env_ids: Sequence[int] | None):
-        if env_ids is None:
-            env_ids = self.robot._ALL_INDICES
+    def _reset_idx(self, env_ids: torch.Tensor | None):
+        if env_ids is None or len(env_ids) == self.num_envs:
+            env_ids = self._robot._ALL_INDICES
+        self._robot.reset(env_ids)
         super()._reset_idx(env_ids)
+        if len(env_ids) == self.num_envs:
+            # Spread out the resets to avoid spikes in training when many environments reset at a similar time
+            self.episode_length_buf[:] = torch.randint_like(self.episode_length_buf, high=int(self.max_episode_length))
+        self._actions[env_ids] = 0.0
+        self._previous_actions[env_ids] = 0.0
+        # Sample new commands (only x/y)
+        self._commands[env_ids] = torch.zeros_like(self._commands[env_ids]).uniform_(-1.0, 1.0)
+        # Reset robot state
+        joint_pos = self._robot.data.default_joint_pos[env_ids]
+        #randomize joint positions by adding a number in [-0.3, 0.3] uniformly
+        joint_pos += torch.rand_like(joint_pos) * 0.6 - 0.3
+        joint_vel = self._robot.data.default_joint_vel[env_ids]
+        default_root_state = self._robot.data.default_root_state[env_ids]
+        default_root_state[:, :3] += self._terrain.env_origins[env_ids]
+        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
+        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
+        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
+        # Initialize history buffer with repeated current joint_pos
+        self._joint_pos_buffer[env_ids] = joint_pos.unsqueeze(1).repeat(1, self.cfg.history_length, 1)
+        # Randomize time step counter for variety
+        self._time_step_counter[env_ids] = torch.rand(len(env_ids), device=self.device) * self.cfg.phase1_period
+        # Logging
+        extras = dict()
+        for key in self._episode_sums.keys():
+            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])
+            extras["Episode_Reward/" + key] = episodic_sum_avg / self.max_episode_length_s
+            self._episode_sums[key][env_ids] = 0.0
+        self.extras["log"] = dict()
+        self.extras["log"].update(extras)
+        extras = dict()
+        extras["Episode_Termination/leg_contact"] = torch.count_nonzero(self.reset_terminated[env_ids]).item()
+        extras["Episode_Termination/time_out"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()
+        self.extras["log"].update(extras)
 
-        joint_pos = self.robot.data.default_joint_pos[env_ids]
-        joint_pos[:, self._pole_dof_idx] += sample_uniform(
-            self.cfg.initial_pole_angle_range[0] * math.pi,
-            self.cfg.initial_pole_angle_range[1] * math.pi,
-            joint_pos[:, self._pole_dof_idx].shape,
-            joint_pos.device,
-        )
-        joint_vel = self.robot.data.default_joint_vel[env_ids]
-
-        default_root_state = self.robot.data.default_root_state[env_ids]
-        default_root_state[:, :3] += self.scene.env_origins[env_ids]
-
-        self.joint_pos[env_ids] = joint_pos
-        self.joint_vel[env_ids] = joint_vel
-
-        self.robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self.robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self.robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-
-@torch.jit.script
-def compute_rewards(
-    rew_scale_alive: float,
-    rew_scale_terminated: float,
-    rew_scale_pole_pos: float,
-    rew_scale_cart_vel: float,
-    rew_scale_pole_vel: float,
-    pole_pos: torch.Tensor,
-    pole_vel: torch.Tensor,
-    cart_pos: torch.Tensor,
-    cart_vel: torch.Tensor,
-    reset_terminated: torch.Tensor,
-):
-    rew_alive = rew_scale_alive * (1.0 - reset_terminated.float())
-    rew_termination = rew_scale_terminated * reset_terminated.float()
-    rew_pole_pos = rew_scale_pole_pos * torch.sum(torch.square(pole_pos).unsqueeze(dim=1), dim=-1)
-    rew_cart_vel = rew_scale_cart_vel * torch.sum(torch.abs(cart_vel).unsqueeze(dim=1), dim=-1)
-    rew_pole_vel = rew_scale_pole_vel * torch.sum(torch.abs(pole_vel).unsqueeze(dim=1), dim=-1)
-    total_reward = rew_alive + rew_termination + rew_pole_pos + rew_cart_vel + rew_pole_vel
-    return total_reward
\ No newline at end of file
+    def step(self, actions: torch.Tensor):
+        """Extends the base step to update visualization markers."""
+        observations, rewards, terminated, truncated, extras = super().step(actions)
+        self._time_step_counter += self.cfg.sim.dt * self.cfg.decimation
+        # if self.cfg.debug_vis:
+        #     self._update_markers()
+        return observations, rewards, terminated, truncated, extras
+
+    def _update_markers(self):
+        """Update visualization markers for commanded and actual directions."""
+        root_pos_w = self._robot.data.root_pos_w.clone()
+        root_quat_w = self._robot.data.root_quat_w
+
+        # Helper function to compute quats aligning z-axis (cone direction) to the velocity vector
+        def compute_cone_quats(dirs: torch.Tensor) -> torch.Tensor:
+            e_z = torch.tensor([0.0, 0.0, 1.0], device=self.device).expand(dirs.shape[0], -1)
+            dot = torch.sum(e_z * dirs, dim=-1)
+            cross = torch.cross(e_z, dirs, dim=-1)
+            cross_norm = torch.norm(cross, dim=-1)
+            angle = torch.atan2(cross_norm, dot)
+            axis = cross / cross_norm.clamp_min(1e-6).unsqueeze(-1)
+            quats = quat_from_angle_axis(angle, axis)
+
+            # Handle parallel cases
+            mask_parallel = dot > 0.999
+            quats[mask_parallel] = torch.tensor([1.0, 0.0, 0.0, 0.0], device=self.device).expand(mask_parallel.sum(), -1)
+
+            # Handle antiparallel cases
+            mask_anti = dot < -0.999
+            if torch.any(mask_anti):
+                perp_axis = torch.cross(torch.tensor([1.0, 0.0, 0.0], device=self.device).expand(mask_anti.sum(), -1), dirs[mask_anti], dim=-1)
+                perp_axis /= torch.norm(perp_axis, dim=-1).unsqueeze(-1).clamp_min(1e-6)
+                quats[mask_anti] = quat_from_angle_axis(torch.full((mask_anti.sum(),), torch.pi, device=self.device), perp_axis)
+
+            return quats
+
+        # Commanded direction (in world frame)
+        commanded_vel_b = torch.cat((self._commands, torch.zeros(self.num_envs, 1, device=self.device)), dim=1)
+        commanded_dir_w = quat_rotate(root_quat_w, commanded_vel_b)
+        commanded_norm = torch.norm(commanded_dir_w, dim=-1)
+        mask_cmd = commanded_norm > 0.1
+        if torch.any(mask_cmd):
+            norm_dir_w = commanded_dir_w[mask_cmd] / commanded_norm[mask_cmd].unsqueeze(-1)
+            arrow_pos_cmd = root_pos_w[mask_cmd]
+            arrow_pos_cmd[:, 2] += 0.1  # Offset above ground
+            cone_quats_cmd = compute_cone_quats(norm_dir_w)
+            self._commanded_markers.visualize(arrow_pos_cmd, cone_quats_cmd)
+
+        # Hide if no visible
+        if not torch.any(mask_cmd):
+            self._commanded_markers.visualize(torch.empty(0, 3, device=self.device), torch.empty(0, 4, device=self.device))
+
+        # Actual direction (in world frame)
+        actual_vel_w = self._robot.data.root_lin_vel_w[:, :3]
+        actual_norm = torch.norm(actual_vel_w, dim=-1)
+        mask_act = actual_norm > 0.1
+        if torch.any(mask_act):
+            norm_dir_w = actual_vel_w[mask_act] / actual_norm[mask_act].unsqueeze(-1)
+            arrow_pos_act = root_pos_w[mask_act]
+            arrow_pos_act[:, 2] += 0.1
+            cone_quats_act = compute_cone_quats(norm_dir_w)
+            self._actual_markers.visualize(arrow_pos_act, cone_quats_act)
+
+        if not torch.any(mask_act):
+            self._actual_markers.visualize(torch.empty(0, 3, device=self.device), torch.empty(0, 4, device=self.device))
\ No newline at end of file
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
index fd2dad3..f514ef2 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
@@ -1,48 +1,115 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# spdrbot3_env_cfg.py
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
 
-from isaaclab_assets.robots.cartpole import CARTPOLE_CFG
-
+import isaaclab.envs.mdp as mdp
+import isaaclab.sim as sim_utils
 from isaaclab.assets import ArticulationCfg
 from isaaclab.envs import DirectRLEnvCfg
+from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.managers import SceneEntityCfg
 from isaaclab.scene import InteractiveSceneCfg
 from isaaclab.sim import SimulationCfg
+from isaaclab.terrains import TerrainImporterCfg
 from isaaclab.utils import configclass
+from isaaclab.sensors import ContactSensorCfg
+
+from isaaclab_assets.robots.spdrbot import SPDRBOT_CFG  # isort: skip
+
+
+@configclass
+class EventCfg:
+    """Configuration for randomization."""
+
+    physics_material = EventTerm(
+        func=mdp.randomize_rigid_body_material,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
+            "static_friction_range": (0.8, 0.8),
+            "dynamic_friction_range": (0.6, 0.6),
+            "restitution_range": (0.0, 0.0),
+            "num_buckets": 64,
+        },
+    )
+    add_base_mass = EventTerm(
+        func=mdp.randomize_rigid_body_mass,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names="base_link"),
+            "mass_distribution_params": (0.2, 1.0),
+            "operation": "add",
+        },
+    )
 
 
 @configclass
 class Spdrbot3EnvCfg(DirectRLEnvCfg):
     # env
-    decimation = 2
-    episode_length_s = 5.0
-    # - spaces definition
-    action_space = 1
-    observation_space = 4
+    episode_length_s = 40.0
+    decimation = 8
+    action_scale = 1
+    action_space = 12
+    history_length = 5  # Number of past joint position snapshots (including current)
+    phase1_period = 5.0  # Sine wave period for phase 1, frequency = 0.2 Hz
+    observation_space = 2 + 12 * history_length + 2  # 2 commands + joints history + sin/cos phase * 2
     state_space = 0
 
+    debug_vis: bool = True  # Enable visualization markers
+
     # simulation
-    sim: SimulationCfg = SimulationCfg(dt=1 / 120, render_interval=decimation)
+    sim: SimulationCfg = SimulationCfg(
+        dt=1 / 200,
+        render_interval=1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=1.0,
+            restitution=0.0,
+        ),
+    )
+    terrain = TerrainImporterCfg(
+        prim_path="/World/ground",
+        terrain_type="plane",
+        collision_group=-1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=1.0,
+            restitution=0.0,
+        ),
+        debug_vis=False,
+    )
 
-    # robot(s)
-    robot_cfg: ArticulationCfg = CARTPOLE_CFG.replace(prim_path="/World/envs/env_.*/Robot")
+    contact_sensor: ContactSensorCfg = ContactSensorCfg(
+        prim_path="/World/envs/env_.*/Robot/.*",  # Monitor all bodies
+        history_length=5,
+        update_period=0.005,  # Matches sim dt = 1/200
+        track_air_time=True,  # Enable for feet air time
+    )
 
     # scene
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=4096, env_spacing=4.0, replicate_physics=True)
-
-    # custom parameters/scales
-    # - controllable joint
-    cart_dof_name = "slider_to_cart"
-    pole_dof_name = "cart_to_pole"
-    # - action scale
-    action_scale = 100.0  # [N]
-    # - reward scales
-    rew_scale_alive = 1.0
-    rew_scale_terminated = -2.0
-    rew_scale_pole_pos = -1.0
-    rew_scale_cart_vel = -0.01
-    rew_scale_pole_vel = -0.005
-    # - reset states/conditions
-    initial_pole_angle_range = [-0.25, 0.25]  # pole angle sample range on reset [rad]
-    max_cart_pos = 3.0  # reset if cart exceeds this position [m]
\ No newline at end of file
+    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=20000, env_spacing=1.0, replicate_physics=True)
+
+    # events
+    events: EventCfg = EventCfg()
+
+    # robot
+    robot: ArticulationCfg = SPDRBOT_CFG.replace(prim_path="/World/envs/env_.*/Robot")
+
+    # reward scales
+    lin_vel_reward_scale = 20.0  # Increased for more movement reward
+    yaw_rate_reward_scale = 0
+    z_vel_reward_scale = 0
+    ang_vel_reward_scale = -0.05
+    joint_vel_penalty_scale = -0.003
+    joint_torque_reward_scale = -1e-4  # Reduced punishment
+    joint_accel_reward_scale = -1e-5  # Reduced punishment
+    action_rate_reward_scale = -0.01
+    flat_orientation_reward_scale = -2.0
+    max_tilt_angle_deg = 45.0
+    power_penalty_scale = -1e-4
\ No newline at end of file