--- git status ---
On branch master
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
	modified:   source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
index 44bdf1e..864f3ca 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
@@ -1,4 +1,5 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/__init__.py
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers[](https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
index b7e7cc9..fb5ec27 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/agents/rsl_rl_ppo_cfg.py
@@ -10,8 +10,8 @@ from isaaclab_rl.rsl_rl import RslRlOnPolicyRunnerCfg, RslRlPpoActorCriticCfg, R
 
 @configclass
 class PPORunnerCfg(RslRlOnPolicyRunnerCfg):
-    num_steps_per_env = 16
-    max_iterations = 150
+    num_steps_per_env = 24
+    max_iterations = 20000
     save_interval = 50
     experiment_name = "cartpole_direct"
     empirical_normalization = False
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
index b50c7a5..d698b0b 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env.py
@@ -1,19 +1,18 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# Updated spdrbot3_env.py with adjustments for space dims
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
 
 from __future__ import annotations
 
-import math
+import gymnasium as gym
 import torch
-from collections.abc import Sequence
-
+import math
 import isaaclab.sim as sim_utils
 from isaaclab.assets import Articulation
 from isaaclab.envs import DirectRLEnv
-from isaaclab.sim.spawners.from_files import GroundPlaneCfg, spawn_ground_plane
-from isaaclab.utils.math import sample_uniform
+from isaaclab.sensors import ContactSensor
 
 from .spdrbot3_env_cfg import Spdrbot3EnvCfg
 
@@ -24,112 +23,160 @@ class Spdrbot3Env(DirectRLEnv):
     def __init__(self, cfg: Spdrbot3EnvCfg, render_mode: str | None = None, **kwargs):
         super().__init__(cfg, render_mode, **kwargs)
 
-        self._cart_dof_idx, _ = self.robot.find_joints(self.cfg.cart_dof_name)
-        self._pole_dof_idx, _ = self.robot.find_joints(self.cfg.pole_dof_name)
-
-        self.joint_pos = self.robot.data.joint_pos
-        self.joint_vel = self.robot.data.joint_vel
+        # Use gymnasium.spaces.flatdim to get dimensions from spaces
+        action_dim = gym.spaces.flatdim(self.single_action_space)
+        # Joint position command (deviation from default joint positions)
+        self._actions = torch.zeros(self.num_envs, action_dim, device=self.device)
+        self._previous_actions = torch.zeros_like(self._actions)
+
+        # X/Y linear velocity and yaw angular velocity commands
+        self._commands = torch.zeros(self.num_envs, 3, device=self.device)
+
+        # Joint position history buffer (previous 3 steps, 12 joints)
+        self.joint_pos_history = torch.zeros(self.num_envs, 3, self._robot.num_joints, device=self.device)
+
+        # Get specific body indices
+        self._base_id, _ = self._contact_sensor.find_bodies("base_link")
+
+        # Logging (kept original keys)
+        self._episode_sums = {
+            key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
+            for key in [
+                "track_lin_vel_xy_exp",
+                "track_ang_vel_z_exp",
+                "lin_vel_z_l2",
+                "ang_vel_xy_l2",
+                "dof_torques_l2",
+                "dof_acc_l2",
+                "action_rate_l2",
+                "flat_orientation_l2",
+                "stationary_penalty",
+                "base_contact_penalty",
+            ]
+        }
 
     def _setup_scene(self):
-        self.robot = Articulation(self.cfg.robot_cfg)
-        # add ground plane
-        spawn_ground_plane(prim_path="/World/ground", cfg=GroundPlaneCfg())
+        self._robot = Articulation(self.cfg.robot)
+        self.scene.articulations["robot"] = self._robot
+        self._contact_sensor = ContactSensor(self.cfg.contact_sensor)
+        self.scene.sensors["contact_sensor"] = self._contact_sensor
+        self.cfg.terrain.num_envs = self.scene.cfg.num_envs
+        self.cfg.terrain.env_spacing = self.scene.cfg.env_spacing
+        self._terrain = self.cfg.terrain.class_type(self.cfg.terrain)
         # clone and replicate
         self.scene.clone_environments(copy_from_source=False)
-        # we need to explicitly filter collisions for CPU simulation
+        # explicitly filter collisions for CPU simulation
         if self.device == "cpu":
-            self.scene.filter_collisions(global_prim_paths=[])
-        # add articulation to scene
-        self.scene.articulations["robot"] = self.robot
+            self.scene.filter_collisions(global_prim_paths=[self.cfg.terrain.prim_path])
         # add lights
         light_cfg = sim_utils.DomeLightCfg(intensity=2000.0, color=(0.75, 0.75, 0.75))
         light_cfg.func("/World/Light", light_cfg)
 
-    def _pre_physics_step(self, actions: torch.Tensor) -> None:
-        self.actions = actions.clone()
+    def _pre_physics_step(self, actions: torch.Tensor):
+        self._actions = actions.clone()
+        self._processed_actions = self.cfg.action_scale * self._actions + self._robot.data.default_joint_pos
 
-    def _apply_action(self) -> None:
-        self.robot.set_joint_effort_target(self.actions * self.cfg.action_scale, joint_ids=self._cart_dof_idx)
+    def _apply_action(self):
+        self._robot.set_joint_position_target(self._processed_actions)
 
     def _get_observations(self) -> dict:
+        self._previous_actions = self._actions.clone()
+        # Current relative joint positions
+        current_joint_pos = self._robot.data.joint_pos - self._robot.data.default_joint_pos
+
+        # Compute sine and cosine waves at 0.2 Hz (sin/cos for phase; single frequency is sufficient for basic oscillation learning)
+        phase = 2 * math.pi * 0.2 * (self.episode_length_buf * self.step_dt)
+        phase_sin = torch.sin(phase).unsqueeze(-1)
+        phase_cos = torch.cos(phase).unsqueeze(-1)
+
+        # Concatenate: projected gravity + commands + flattened joint history + sin/cos
         obs = torch.cat(
-            (
-                self.joint_pos[:, self._pole_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_vel[:, self._pole_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_pos[:, self._cart_dof_idx[0]].unsqueeze(dim=1),
-                self.joint_vel[:, self._cart_dof_idx[0]].unsqueeze(dim=1),
-            ),
+            [
+                self._robot.data.projected_gravity_b,
+                self._commands,
+                self.joint_pos_history.view(self.num_envs, -1),
+                phase_sin,
+                phase_cos,
+            ],
             dim=-1,
         )
+
+        # Update history (after use)
+        self.joint_pos_history[:, :-1] = self.joint_pos_history[:, 1:]
+        self.joint_pos_history[:, -1] = current_joint_pos
+
         observations = {"policy": obs}
         return observations
 
     def _get_rewards(self) -> torch.Tensor:
-        total_reward = compute_rewards(
-            self.cfg.rew_scale_alive,
-            self.cfg.rew_scale_terminated,
-            self.cfg.rew_scale_pole_pos,
-            self.cfg.rew_scale_cart_vel,
-            self.cfg.rew_scale_pole_vel,
-            self.joint_pos[:, self._pole_dof_idx[0]],
-            self.joint_vel[:, self._pole_dof_idx[0]],
-            self.joint_pos[:, self._cart_dof_idx[0]],
-            self.joint_vel[:, self._cart_dof_idx[0]],
-            self.reset_terminated,
-        )
-        return total_reward
+        # Full rewards from original for robustness
+        lin_vel_error = torch.sum(torch.square(self._commands[:, :2] - self._robot.data.root_lin_vel_b[:, :2]), dim=1)
+        lin_vel_error_mapped = torch.exp(-lin_vel_error / 0.25)
+        yaw_rate_error = torch.square(self._commands[:, 2] - self._robot.data.root_ang_vel_b[:, 2])
+        yaw_rate_error_mapped = torch.exp(-yaw_rate_error / 0.25)
+        z_vel_error = torch.square(self._robot.data.root_lin_vel_b[:, 2])
+        ang_vel_error = torch.sum(torch.square(self._robot.data.root_ang_vel_b[:, :2]), dim=1)
+        joint_torques = torch.sum(torch.square(self._robot.data.applied_torque), dim=1)
+        joint_accel = torch.sum(torch.square(self._robot.data.joint_acc), dim=1)
+        action_rate = torch.sum(torch.square(self._actions - self._previous_actions), dim=1)
+        flat_orientation = torch.sum(torch.square(self._robot.data.projected_gravity_b[:, :2]), dim=1)
+        lin_vel_norm = torch.norm(self._robot.data.root_lin_vel_b[:, :2], dim=1)
+        stationary = (lin_vel_norm < 0.1).float() * self.cfg.stationary_penalty_scale * self.step_dt
+        net_contact_forces = self._contact_sensor.data.net_forces_w_history
+        is_base_contact = (
+            torch.max(torch.norm(net_contact_forces[:, :, self._base_id], dim=-1), dim=1)[0] > 1.0
+        ).float() * self.cfg.base_contact_penalty_scale * self.step_dt
+        is_base_contact = is_base_contact.squeeze(-1)
+
+        rewards = {
+            "track_lin_vel_xy_exp": lin_vel_error_mapped * self.cfg.lin_vel_reward_scale * self.step_dt,
+            "track_ang_vel_z_exp": yaw_rate_error_mapped * self.cfg.yaw_rate_reward_scale * self.step_dt,
+            "lin_vel_z_l2": z_vel_error * self.cfg.z_vel_reward_scale * self.step_dt,
+            "ang_vel_xy_l2": ang_vel_error * self.cfg.ang_vel_reward_scale * self.step_dt,
+            "dof_torques_l2": joint_torques * self.cfg.joint_torque_reward_scale * self.step_dt,
+            "dof_acc_l2": joint_accel * self.cfg.joint_accel_reward_scale * self.step_dt,
+            "action_rate_l2": action_rate * self.cfg.action_rate_reward_scale * self.step_dt,
+            "flat_orientation_l2": flat_orientation * self.cfg.flat_orientation_reward_scale * self.step_dt,
+            "stationary_penalty": stationary,
+            "base_contact_penalty": is_base_contact,
+        }
+        reward = torch.sum(torch.stack(list(rewards.values())), dim=0)
+        # Logging
+        for key, value in rewards.items():
+            self._episode_sums[key] += value
+        return reward
 
     def _get_dones(self) -> tuple[torch.Tensor, torch.Tensor]:
-        self.joint_pos = self.robot.data.joint_pos
-        self.joint_vel = self.robot.data.joint_vel
-
         time_out = self.episode_length_buf >= self.max_episode_length - 1
-        out_of_bounds = torch.any(torch.abs(self.joint_pos[:, self._cart_dof_idx]) > self.cfg.max_cart_pos, dim=1)
-        out_of_bounds = out_of_bounds | torch.any(torch.abs(self.joint_pos[:, self._pole_dof_idx]) > math.pi / 2, dim=1)
-        return out_of_bounds, time_out
+        died = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
+        return died, time_out
 
-    def _reset_idx(self, env_ids: Sequence[int] | None):
-        if env_ids is None:
-            env_ids = self.robot._ALL_INDICES
+    def _reset_idx(self, env_ids: torch.Tensor | None):
+        if env_ids is None or len(env_ids) == self.num_envs:
+            env_ids = self._robot._ALL_INDICES
+        self._robot.reset(env_ids)
         super()._reset_idx(env_ids)
-
-        joint_pos = self.robot.data.default_joint_pos[env_ids]
-        joint_pos[:, self._pole_dof_idx] += sample_uniform(
-            self.cfg.initial_pole_angle_range[0] * math.pi,
-            self.cfg.initial_pole_angle_range[1] * math.pi,
-            joint_pos[:, self._pole_dof_idx].shape,
-            joint_pos.device,
-        )
-        joint_vel = self.robot.data.default_joint_vel[env_ids]
-
-        default_root_state = self.robot.data.default_root_state[env_ids]
-        default_root_state[:, :3] += self.scene.env_origins[env_ids]
-
-        self.joint_pos[env_ids] = joint_pos
-        self.joint_vel[env_ids] = joint_vel
-
-        self.robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
-        self.robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
-        self.robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
-
-
-@torch.jit.script
-def compute_rewards(
-    rew_scale_alive: float,
-    rew_scale_terminated: float,
-    rew_scale_pole_pos: float,
-    rew_scale_cart_vel: float,
-    rew_scale_pole_vel: float,
-    pole_pos: torch.Tensor,
-    pole_vel: torch.Tensor,
-    cart_pos: torch.Tensor,
-    cart_vel: torch.Tensor,
-    reset_terminated: torch.Tensor,
-):
-    rew_alive = rew_scale_alive * (1.0 - reset_terminated.float())
-    rew_termination = rew_scale_terminated * reset_terminated.float()
-    rew_pole_pos = rew_scale_pole_pos * torch.sum(torch.square(pole_pos).unsqueeze(dim=1), dim=-1)
-    rew_cart_vel = rew_scale_cart_vel * torch.sum(torch.abs(cart_vel).unsqueeze(dim=1), dim=-1)
-    rew_pole_vel = rew_scale_pole_vel * torch.sum(torch.abs(pole_vel).unsqueeze(dim=1), dim=-1)
-    total_reward = rew_alive + rew_termination + rew_pole_pos + rew_cart_vel + rew_pole_vel
-    return total_reward
\ No newline at end of file
+        if len(env_ids) == self.num_envs:
+            self.episode_length_buf[:] = torch.randint_like(self.episode_length_buf, high=int(self.max_episode_length))
+        self._actions[env_ids] = 0.0
+        self._previous_actions[env_ids] = 0.0
+        self.joint_pos_history[env_ids] = 0.0
+        self._commands[env_ids] = torch.zeros_like(self._commands[env_ids]).uniform_(-1.0, 1.0)
+        joint_pos = self._robot.data.default_joint_pos[env_ids]
+        joint_vel = self._robot.data.default_joint_vel[env_ids]
+        default_root_state = self._robot.data.default_root_state[env_ids]
+        default_root_state[:, :3] += self._terrain.env_origins[env_ids]
+        self._robot.write_root_pose_to_sim(default_root_state[:, :7], env_ids)
+        self._robot.write_root_velocity_to_sim(default_root_state[:, 7:], env_ids)
+        self._robot.write_joint_state_to_sim(joint_pos, joint_vel, None, env_ids)
+        # Logging
+        extras = dict()
+        for key in self._episode_sums.keys():
+            episodic_sum_avg = torch.mean(self._episode_sums[key][env_ids])
+            extras["Episode Reward/" + key] = episodic_sum_avg / self.max_episode_length_s
+            self._episode_sums[key][env_ids] = 0.0
+        self.extras["log"] = dict()
+        self.extras["log"].update(extras)
+        extras = dict()
+        extras["Episode Termination/time_out"] = torch.count_nonzero(self.reset_time_outs[env_ids]).item()
+        self.extras["log"].update(extras)
\ No newline at end of file
diff --git a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
index fd2dad3..5e619ec 100644
--- a/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
+++ b/source/spdrbot3/spdrbot3/tasks/direct/spdrbot3/spdrbot3_env_cfg.py
@@ -1,48 +1,113 @@
-# Copyright (c) 2022-2025, The Isaac Lab Project Developers (https://github.com/isaac-sim/IsaacLab/blob/main/CONTRIBUTORS.md).
+# Updated spdrbot3_env_cfg.py for Isaac Lab 2.0 compatibility
+# Copyright (c) 2022-2025, The Isaac Lab Project Developers.
 # All rights reserved.
 #
 # SPDX-License-Identifier: BSD-3-Clause
 
-from isaaclab_assets.robots.cartpole import CARTPOLE_CFG
-
+import gymnasium as gym
+import isaaclab.envs.mdp as mdp
+import isaaclab.sim as sim_utils
 from isaaclab.assets import ArticulationCfg
 from isaaclab.envs import DirectRLEnvCfg
+from isaaclab.managers import EventTermCfg as EventTerm
+from isaaclab.managers import SceneEntityCfg
 from isaaclab.scene import InteractiveSceneCfg
 from isaaclab.sim import SimulationCfg
+from isaaclab.terrains import TerrainImporterCfg
 from isaaclab.utils import configclass
+from isaaclab.sensors import ContactSensorCfg
+
+from isaaclab_assets.robots.spdrbot import SPDRBOT_CFG  # isort: skip
+
+
+@configclass
+class EventCfg:
+    """Configuration for randomization."""
+
+    physics_material = EventTerm(
+        func=mdp.randomize_rigid_body_material,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names=".*"),
+            "static_friction_range": (0.8, 0.8),
+            "dynamic_friction_range": (0.6, 0.6),
+            "restitution_range": (0.0, 0.0),
+            "num_buckets": 64,
+        },
+    )
+    add_base_mass = EventTerm(
+        func=mdp.randomize_rigid_body_mass,
+        mode="startup",
+        params={
+            "asset_cfg": SceneEntityCfg("robot", body_names="base_link"),
+            "mass_distribution_params": (1.0, 3.0),
+            "operation": "add",
+        },
+    )
 
 
 @configclass
 class Spdrbot3EnvCfg(DirectRLEnvCfg):
     # env
-    decimation = 2
-    episode_length_s = 5.0
-    # - spaces definition
-    action_space = 1
-    observation_space = 4
-    state_space = 0
+    episode_length_s = 20.0
+    decimation = 4
+    action_scale = 1
+    # Use full gymnasium spaces instead of deprecated num_* attributes
+    action_space = gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(12,))
+    observation_space = gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(44,))  # projected_gravity (3) + commands (3) + joint_pos_history (3*12=36) + sin/cos (2) = 44
+    state_space = gym.spaces.Box(low=-float("inf"), high=float("inf"), shape=(0,))  # Empty state space since not used
 
     # simulation
-    sim: SimulationCfg = SimulationCfg(dt=1 / 120, render_interval=decimation)
+    sim: SimulationCfg = SimulationCfg(
+        dt=1 / 200,
+        render_interval=1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=1.0,
+            restitution=0.0,
+        ),
+    )
+    terrain = TerrainImporterCfg(
+        prim_path="/World/ground",
+        terrain_type="plane",
+        collision_group=-1,
+        physics_material=sim_utils.RigidBodyMaterialCfg(
+            friction_combine_mode="multiply",
+            restitution_combine_mode="multiply",
+            static_friction=1.0,
+            dynamic_friction=1.0,
+            restitution=0.0,
+        ),
+        debug_vis=False,
+    )
 
-    # robot(s)
-    robot_cfg: ArticulationCfg = CARTPOLE_CFG.replace(prim_path="/World/envs/env_.*/Robot")
+    contact_sensor: ContactSensorCfg = ContactSensorCfg(
+        prim_path="/World/envs/env_.*/Robot/base_link",  # Adjusted path
+        history_length=3,
+        update_period=0.005,  # Matches sim dt
+        track_air_time=False,
+    )
 
     # scene
-    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=4096, env_spacing=4.0, replicate_physics=True)
-
-    # custom parameters/scales
-    # - controllable joint
-    cart_dof_name = "slider_to_cart"
-    pole_dof_name = "cart_to_pole"
-    # - action scale
-    action_scale = 100.0  # [N]
-    # - reward scales
-    rew_scale_alive = 1.0
-    rew_scale_terminated = -2.0
-    rew_scale_pole_pos = -1.0
-    rew_scale_cart_vel = -0.01
-    rew_scale_pole_vel = -0.005
-    # - reset states/conditions
-    initial_pole_angle_range = [-0.25, 0.25]  # pole angle sample range on reset [rad]
-    max_cart_pos = 3.0  # reset if cart exceeds this position [m]
\ No newline at end of file
+    scene: InteractiveSceneCfg = InteractiveSceneCfg(num_envs=200, env_spacing=2.0, replicate_physics=True)
+
+    # events
+    events: EventCfg = EventCfg()
+
+    # robot
+    robot: ArticulationCfg = SPDRBOT_CFG.replace(prim_path="/World/envs/env_.*/Robot")
+
+    # reward scales
+    lin_vel_reward_scale = 5.0
+    yaw_rate_reward_scale = 0.5
+    z_vel_reward_scale = -2.0
+    ang_vel_reward_scale = -0.05
+    joint_torque_reward_scale = -1e-5
+    joint_accel_reward_scale = -1e-7
+    action_rate_reward_scale = -0.01
+    flat_orientation_reward_scale = -5.0
+    stationary_penalty_scale = -0.1
+    max_tilt_angle_deg = 45.0
+    base_contact_penalty_scale = -2.0
\ No newline at end of file